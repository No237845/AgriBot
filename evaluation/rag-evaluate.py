# =========================================================
# üåæ AgriBot Burkina - √âvaluation du syst√®me RAG
# =========================================================
# Ce script √©value ton mod√®le RAG selon trois crit√®res :
#   1Ô∏è‚É£ Pertinence des documents r√©cup√©r√©s
#   2Ô∏è‚É£ Fid√©lit√© de la r√©ponse (absence d‚Äôhallucination)
#   3Ô∏è‚É£ Exactitude de la r√©ponse (par rapport √† la v√©rit√© attendue)
#
# ‚öôÔ∏è 100 % open-source : utilise des mod√®les SentenceTransformer,
# BERTScore et ROUGE sans d√©pendance OpenAI.
#
# Auteur : Kabore Innocent
# =========================================================

import os
import pandas as pd
import matplotlib.pyplot as plt
from sentence_transformers import SentenceTransformer, util
from evaluate import load

# =========================================================
# 1Ô∏è‚É£ CHARGEMENT DES DONN√âES
# =========================================================
# Le fichier d‚Äôentr√©e doit contenir au minimum :
# - "question" : la question pos√©e √† ton RAG
# - "retrieved_doc" : le passage extrait par le retriever
# - "generated_answer" : la r√©ponse produite par ton mod√®le
# - "expected_answer" : la r√©ponse de r√©f√©rence (si disponible)

EVAL_DATA_PATH = "./evaluation/rag_eval_dataset.csv"

if not os.path.exists(EVAL_DATA_PATH):
    raise FileNotFoundError(f"‚ùå Fichier d‚Äô√©valuation introuvable : {EVAL_DATA_PATH}")

df = pd.read_csv(EVAL_DATA_PATH)
print(f"‚úÖ Donn√©es charg√©es : {len(df)} exemples\n")

# =========================================================
# 2Ô∏è‚É£ √âVALUATION DE LA PERTINENCE (retrieval relevance)
# =========================================================
# On mesure la similarit√© entre la question et le document
# r√©cup√©r√© √† l‚Äôaide d‚Äôun mod√®le de similarit√© s√©mantique.
# Valeurs proches de 1 = document tr√®s pertinent.

print("üîπ √âvaluation de la pertinence des documents...")

model_sim = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

def relevance_score(question, retrieved_doc):
    emb = model_sim.encode([question, retrieved_doc])
    return float(util.cos_sim(emb[0], emb[1]))

df["relevance"] = df.apply(
    lambda r: relevance_score(r["question"], r["retrieved_doc"]), axis=1
)

# =========================================================
# 3Ô∏è‚É£ √âVALUATION DE LA FID√âLIT√â (faithfulness)
# =========================================================
# On utilise BERTScore pour mesurer la coh√©rence entre
# la r√©ponse g√©n√©r√©e et le document source. Cela permet de
# d√©tecter les "hallucinations".
# Valeur proche de 1 = r√©ponse fid√®le au contexte.

print("üîπ √âvaluation de la fid√©lit√© des r√©ponses...")

bertscore = load("bertscore")
faithfulness = bertscore.compute(
    predictions=df["generated_answer"].tolist(),
    references=df["retrieved_doc"].tolist(),
    lang="fr"
)
df["faithfulness"] = faithfulness["f1"]

# =========================================================
# 4Ô∏è‚É£ √âVALUATION DE L‚ÄôEXACTITUDE (answer correctness)
# =========================================================
# On compare la r√©ponse g√©n√©r√©e √† la "bonne" r√©ponse
# attendue avec la m√©trique ROUGE-L.
# Plus le score est haut, plus la r√©ponse est correcte.

print("üîπ √âvaluation de l‚Äôexactitude des r√©ponses...")

rouge = load("rouge")
rouge_results = rouge.compute(
    predictions=df["generated_answer"].tolist(),
    references=df["expected_answer"].tolist()
)
df["rougeL"] = rouge_results["rougeL"]

# =========================================================
# 5Ô∏è‚É£ COMBINAISON DES M√âTRIQUES
# =========================================================
# Pond√©ration : pertinence (40%) + fid√©lit√© (30%) + exactitude (30%)
# Ce score global permet de classer la performance du syst√®me RAG.

df["global_score"] = (
    0.4 * df["relevance"] +
    0.3 * df["faithfulness"] +
    0.3 * df["rougeL"]
)

# =========================================================
# 6Ô∏è‚É£ ANALYSE & VISUALISATION
# =========================================================
print("\nüìä R√©sum√© des scores moyens :")
print(f"- Pertinence moyenne   : {df['relevance'].mean():.3f}")
print(f"- Fid√©lit√© moyenne     : {df['faithfulness'].mean():.3f}")
print(f"- Exactitude moyenne   : {df['rougeL'].mean():.3f}")
print(f"- Score global moyen   : {df['global_score'].mean():.3f}")

# üîπ Affichage graphique
plt.figure(figsize=(10, 5))
plt.bar(df["question"], df["global_score"], color="#4CAF50")
plt.xticks(rotation=45, ha="right")
plt.title("üåæ √âvaluation globale du syst√®me RAG AgriBot Burkina")
plt.ylabel("Score (0 √† 1)")
plt.tight_layout()
plt.show()

# üîπ Sauvegarde du rapport
OUTPUT_PATH = "./data/eval_results.csv"
df.to_csv(OUTPUT_PATH, index=False)
print(f"\nüìÅ R√©sultats enregistr√©s dans : {OUTPUT_PATH}")